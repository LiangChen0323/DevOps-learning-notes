# DevOps learning notes

## 05/02/20

## **Docker-Kubernetes**

### Rolling updates

Two parameters determine the pace of the rollout: `maxUnavailable` and `maxSurge`  
They can be specified in absolute number of pods, or percentage of the replicas count At any given time ...  
there will always be at least `replicas - maxUnavailable` pods available  
there will never be more than `replicas + maxSurge` pods in total  
there will therefore be up to `maxUnavailable + maxSurge` pods being updated

Rolling updates:

```
Kubectl edit deploy xxx
Or
Kubectl set image deploy xxx
```

```
kubectl describe deployments xxx
```

exp:

```
Replicas: 10 desired | 5 updated | 13 total | 8 available | 5 unavailable
```

to check rollout replicas

```
kubectl rollout undo deploy xxxx
```

to undo rolling update once

```
kubectl rollout status deploy xxxx
```

check rolling update status

```
kubectl rollout history
```

Rollout history

Roll back to the "known good" deployment version:

```
kubectl rollout undo deployment worker --to-revision=1
```

### Healthchecks

Healthchecks are probes that apply to containers (not to pods)  
Each container can have three (optional) probes:
`liveness` = is this container dead or alive? (most important probe)  
`readiness` = is this container ready to serve traffic? (only needed if a service)  
`startup` = is this container still starting up? (alpha in 1.16)  
Different probe handlers are available (HTTP, TCP, program execution)

#### Liveness probe

Indicates if the container is dead or alive  
A dead container cannot come back to life  
If the liveness probe fails, the container is killed  
(to make really sure that it's really dead; no zombies or undeads!)  
What happens next depends on the pod's restartPolicy:  
Never: the container is not restarted  
OnFailure or Always: the container is restarted

#### When to use a liveness probe

To indicate failures that can't be recovered
deadlocks (causing all requests to time out)
internal corruption (causing all requests to error)  
Anything where our incident response would be "just restart/reboot it"  
Do not use liveness probes for problems that can't be fixed by a restart  
Otherwise we just restart our pods for no reason, creating useless load

#### Readiness probe

Indicates if the container is ready to serve traffic  
If a container becomes "unready" it might be ready again soon  
If the readiness probe fails:  
the container is not killed  
if the pod is a member of a service, it is temporarily removed  
it is re-added as soon as the readiness probe passes again

#### When to use a readiness probe

To indicate failure due to an external cause
database is down or unreachable  
mandatory auth or other backend service unavailable  
To indicate temporary failure or unavailability  
application can only service N parallel connections  
runtime is busy doing garbage collection or initial data load

#### Startup probe

Kubernetes 1.16 introduces a third type of probe: startupProbe  
(it is in alpha in Kubernetes 1.16)  
It can be used to indicate "container not ready yet"  
process is still starting  
loading external data, priming caches  
Before Kubernetes 1.16, we had to use the initialDelaySeconds parameter  
(available for both liveness and readiness probes)
initialDelaySeconds is a rigid delay (always wait X before running probes)  
startupProbe works better when a container start time can vary a lot

#### Timing and thresholds

Probes are executed at intervals of periodSeconds (default: 10)  
The timeout for a probe is set with timeoutSeconds (default: 1)  
If a probe takes longer than that, it is considered as a FAIL  
A probe is considered successful after successThreshold successes (default: 1)  
A probe is considered failing after failureThreshold failures (default: 3)  
A probe can have an initialDelaySeconds parameter (default: 0)

#### Questions to ask before adding healthchecks

Do we want liveness, readiness, both?  
(sometimes, we can use the same check, but with different failure thresholds)  
Do we have existing HTTP endpoints that we can use?  
Do we need to add new endpoints, or perhaps use something else?  
Are our healthchecks likely to use resources and/or slow down the app?  
Do they depend on additional services?
